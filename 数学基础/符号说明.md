1. 随机变量，用大写表示，例:
    - 输入随机变量: $X$
    - 输出随机变量: $Y$
2. 实例，用小写表示，例:
    - 输入: $x$
    - 输出: $y$
3. 数据集(m个样本): $\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), (x^{(3)}, y^{(3)}),..., (x^{(m)}, y^{(m)})\}$
    > 其中第i个训练样本为: $(x^{(i)}, y^{(i)})$
4. 针对第i个输入样本$x^{(i)}$，其有n个特征，表示为n维向量: $(x^{(i)}_1, x^{(i)}_2, x^{(i)}_3, ..., x^{(i)}_n)$
    > 其中$x^{(i)}$的第j个特征为$x^{(i)}_j$
5. 成本函数、损失函数
    - $J(\theta)$
6. 最大似然函数
    - $\mathcal{L}(\theta)$
7. 对数最大似然函数
    - $l(\theta) = log \mathcal{L}(\theta) $
8. 梯度(gradient)、雅可比矩阵(Jacobian)、海森矩阵(Hessian)
|$输入 \to 输出$|1阶导数|2阶导数|示例|
|---|---|---|---|
|$\R \to \R$|$\R$|$\R$|$y=x^2$|
|$\R^{n} \to \R$|$gradient \\ \R^{n}$|$Hessian \\ 对称阵\R^{n*n}$|损失函数$J(\theta)$|
|$\R^{m} \to \R^{n}$|$Jacobian \\ \R^{m*n}$|$\R^{m*n*n}$|神经网络中的layer|
    > 参考链接: https://zhuanlan.zhihu.com/p/37306749
9. 参数的3种称呼
|不同角度|机器学习|指数分布族|特定的分布中|
|---|---|---|---|
|之间可相互转化|$\theta$|自然参数$\eta$|$ P(\lambda) \\ N(\mu, \sigma^2) \\ \cdots $|
