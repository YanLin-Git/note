# 几种优化算法

## 一、梯度下降
- 梯度下降
- 随机梯度下降
- 批梯度下降

## 二、Momentum
$$
\bm{v} = \alpha \bm{v} - \eta \frac {\partial L} {\partial \bm{W}}\\
\bm{W} = \bm{W} + \bm{v}
$$

## 三、AdaGrad(Adaptive Gradient)
$$
\bm{h} = \bm{h} + \frac {\partial L} {\partial \bm{W}} \circ \frac {\partial L} {\partial \bm{W}}
$$
> 这里的$\bm{h}$保存了 以前所有梯度值的平方和
$$
\bm{W} = \bm{W} - \eta \frac 1 {\sqrt \bm{h}} \frac {\partial L} {\partial \bm{W}}
$$
> 更新参数时，根据$\bm{h}$调整学习尺度。意味着变动较大的参数，学习率逐渐变小

## 四、RMSProp
- 在AdaGrad基础上，做了进一步改善
> AdaGrad会记录过去所有梯度的平方和。学习越深入，更新幅度越小  
> RMSProp则会逐渐遗忘过去的梯度，更多地关注 较新的梯度

## 五、Adam
- Momentum 与 AdaGrad的结合
