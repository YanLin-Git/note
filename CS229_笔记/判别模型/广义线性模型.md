# 广义线性模型

## 1.1 线性回归(熟悉的最小二乘法)

### 1. 概率解释
首先假设目标变量和输入值存在下面这种等量关系：

$$ 
y^{(i)}=\theta^T x^{(i)}+ \epsilon ^{(i)}
$$

上式中 $ \epsilon ^{(i)}$ 是误差项，用于存放由于建模所忽略的变量导致的效果 （比如可能某些特征对于房价的影响很明显，但我们做回归的时候忽略掉了）或者随机的噪音信息（random noise）。进一步假设 $ \epsilon ^{(i)}$   是独立同分布的 (IID ，independently and identically distributed) ，服从高斯分布（Gaussian distribution ，也叫正态分布 Normal distribution），其平均值为 $0$，方差（variance）为 $\sigma ^2$。这样就可以把这个假设写成 $ \epsilon ^{(i)} ∼ N (0, \sigma ^2)$ 。然后 $ \epsilon ^{(i)} $  的概率密度为：

$$ 
p(\epsilon ^{(i)} )= \frac 1{\sqrt{2\pi}\sigma} exp (- \frac  {(\epsilon ^{(i)} )^2}{2\sigma^2})
$$

也可以理解为$ y^{(i)} ∼ N (\theta^T x^{(i)}, \sigma ^2)$，然后$y^{(i)}$的概率密度为：

$$ 
p(y ^{(i)} |x^{(i)}; \theta)= \frac 1{\sqrt{2\pi}\sigma} exp (- \frac  {(y^{(i)} -\theta^T x ^{(i)} )^2}{2\sigma^2})
$$

> 这里的记号 $“p(y ^{(i)} |x^{(i)}; \theta)”$ 表示的是这是一个对于给定 $x^{(i)}$ 时 $y^{(i)}$ 的分布，用$\theta$ 代表该分布的参数。 注意这里不能写成$p(y ^{(i)} |x^{(i)},\theta)$， 因为 $\theta$ 并不是一个随机变量。

### 2. 最大似然估计
线性回归的似然函数:

$$
\begin{aligned}
    \mathcal{L}(\theta) &= p(\vec{y}|X;\theta)\\
    &= \prod ^m _{i=1}p(y^{(i)}|x^{(i)};\theta)\\
    &= \prod ^m _{i=1} \frac  1{\sqrt{2\pi}\sigma} exp(- \frac {(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2})\\
\end{aligned}
$$

那么相应的对数似然函数为:

$$
\begin{aligned}
l(\theta) &=\log \mathcal{L}(\theta)\\
&=\log \prod ^m _{i=1} \frac  1{\sqrt{2\pi}\sigma} exp(- \frac {(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2})\\
&= \sum ^m _{i=1}log \frac  1{\sqrt{2\pi}\sigma} exp(- \frac {(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2})\\
&= m \log \frac  1{\sqrt{2\pi}\sigma} - \frac 1{\sigma^2}\cdot \frac 12 \sum^m_{i=1} (y^{(i)}-\theta^Tx^{(i)})^2\\
&= 与\theta无关的常量 - \frac 1{\sigma^2}\cdot \frac 12 \sum^m_{i=1} (y^{(i)}-\theta^Tx^{(i)})^2\\
\end{aligned}
$$

所以对数似然函数最大化，就意味着这个子式取最小值:

$$ 
\frac 12 \sum^m _{i=1} (y^{(i)}-\theta^Tx^{(i)})^2 \qquad\qquad = J(\theta)
$$

> 这个式子就是我们常说的均方误差损失函数，即:  
> 线性回归的最大似然估计 == 均方误差损失函数最小化。

### 3. 梯度下降法

#### 随机梯度下降

首先来解决只有一个训练样本 $(x, y)$ 的情况，这样就可以忽略掉等号右边对 $J$ 的求和项目了。损失函数就简化下面这样：

$$ 
J(\theta) = \frac 1 2  (y - \theta^T x)^2
$$

求 $J(\theta)$ 对 $\theta$ 的偏导:

$$
\begin{aligned}
    \frac \partial {\partial\theta_j}J(\theta) & = \frac \partial {\partial\theta_j} \frac  1 2 (y - \theta^T x)^2\\
    & = 2 \cdot\frac 1 2 (y - \theta^T x)\cdot \frac \partial {\partial\theta_j}  (y - \theta^T x) \\
    & = (y - \theta^T x) \cdot \frac \partial {\partial\theta_j}(y - \sum^n_{i=0} \theta_ix_i) \\
    & = - (y - \theta^T x) x_j
\end{aligned}
$$

记$h_\theta(x) = \theta^T x$，于是对单个训练样本，更新规则如下所示：

$$ 
\begin{aligned}
    \theta_j &:= \theta_j + \alpha (y^{(i)}- \theta^T x^{(i)})x_j^{(i)}\\
    &:= \theta_j + \alpha (y^{(i)}-h_\theta (x^{(i)}))x_j^{(i)}
\end{aligned}
$$

#### 批量梯度下降

再考虑有一批样本时，批量梯度下降，就可以写成这样:

$
\begin{aligned}
    &\qquad 重复直到收敛 \{ \\
    &\qquad\qquad\theta_j := \theta_j + \alpha \sum^m_{i=1}(y^{(i)}-h_\theta (x^{(i)}))x_j^{(i)}\quad(对每个j) \\
    &\qquad\}
\end{aligned}
$

## 1.2 逻辑回归
### 1. 概率解释
针对每一组样本(x,y)，首先假设：

$$
\begin{aligned}
    p(y=1|x;\theta) &=  \frac  1{1+e^{-\theta^Tx}}\\
    p(y=0|x;\theta) &= 1 -  \frac  1{1+e^{-\theta^Tx}}\\
\end{aligned}
$$

> 与线性回归中的假设$h_\theta(x) = \theta^T x$相比  
> 这里将$h_\theta(x)$修改为$\frac  1 {1+e^{-\theta^Tx}}$  
> $\theta^T x$的值域为$(-\infty, \infty)$，而$\frac  1 {1+e^{-\theta^Tx}}$则将值域限定在(0,1)

更简洁的写法是：

$$ 
p(y|x;\theta)=(h_\theta (x))^y(1- h_\theta (x))^{1-y}
$$

再修改一下，对于训练集中的第i个样本，我们有:
$$ 
p(y^{(i)}| x^{(i)}; \theta)=(h_\theta (x^{(i)}))^{y^{(i)}}(1-h_\theta (x^{(i)}))^{1-y^{(i)}}
$$


### 2. 最大似然估计
逻辑回归的似然函数:
$$
\begin{aligned}
    \mathcal{L}(\theta) &= p(\vec{y}| X; \theta)\\
    &= \prod^m_{i=1}  p(y^{(i)}| x^{(i)}; \theta)\\
    &= \prod^m_{i=1} (h_\theta (x^{(i)}))^{y^{(i)}}(1-h_\theta (x^{(i)}))^{1-y^{(i)}} \\
\end{aligned}
$$

那么相应的对数似然函数为:

$$
\begin{aligned}
    l(\theta) &=\log \mathcal{L}(\theta) \\
    &= \sum^m_{i=1} y^{(i)} \log h(x^{(i)})+(1-y^{(i)})\log (1-h(x^{(i)}))
\end{aligned}
$$

所以对数似然函数最大化，就意味着这个式子取最小值:

$$ 
    J(\theta) = - \sum^m_{i=1} y^{(i)} \log h(x^{(i)})+(1-y^{(i)})\log (1-h(x^{(i)}))
$$

> 这个式子就是我们常说的逻辑回归损失函数

### 3. 梯度下降法
#### 随机梯度下降
首先来解决只有一个训练样本 $(x, y)$ 的情况，损失函数就简化下面这样：

$$ 
J(\theta) = - y \log h_\theta(x) - (1-y) \log (1-h_\theta(x))
$$

求 $J(\theta)$ 对 $\theta$ 的偏导:

$$
\begin{aligned}
    \frac  {\partial}{\partial \theta_j} J(\theta) &=(-y \frac  1 {h_\theta(x)}  + (1-y) \frac  1 {1- h_\theta(x)} ) \cdot \frac  {\partial}{\partial \theta_j}h_\theta(x) \\
    &= (-y \frac  1 {h_\theta(x)}  + (1-y) \frac  1 {1- h_\theta(x)} ) \cdot  h_\theta(x)(1-h_\theta(x)) \cdot \frac  {\partial}{\partial \theta_j}\theta ^Tx \\
    &= (-y (1-h_\theta(x) ) + (1-y) h_\theta(x)) \cdot x_j\\
    &= - (y-h_\theta(x)) \cdot x_j
\end{aligned}
$$

于是对单个训练样本，更新规则如下所示：

$$ 
\begin{aligned}
    \theta_j := \theta_j + \alpha (y^{(i)}-h_\theta (x^{(i)}))x_j^{(i)}
\end{aligned}
$$

#### 批量梯度下降

再考虑有一批样本时，批量梯度下降，就可以写成这样:

$
\begin{aligned}
    &\qquad 重复直到收敛 \{ \\
    &\qquad\qquad\theta_j := \theta_j + \alpha \sum^m_{i=1}(y^{(i)}-h_\theta (x^{(i)}))x_j^{(i)}\quad(对每个j) \\
    &\qquad\}
\end{aligned}
$

## 1.3 softmax
### 1. 概率解释
在逻辑回归中，针对二分类问题，我们假设：

$$
\begin{aligned}
    p(y=1|x;\theta) &=  \frac  1{1+e^{-\theta^Tx}}\\
    p(y=0|x;\theta) &= 1 -  \frac  1{1+e^{-\theta^Tx}}\\
\end{aligned}
$$

扩展到多分类问题，可做类似假设:
$$
p(y=l|x;\theta) = \frac {e^{\theta_l^Tx}}{\sum^k_{j=1}e^{\theta_j^Tx}}, \quad y \in\{1, ..., k\}
$$
于是，我们可以写出:
$$ 
p(y|x;\theta) = \prod ^k_{l=1} (\frac {e^{\theta_l^Tx}}{\sum^k_{j=1} e^{\theta_j^T x}})^{1\{y^{(i)}=l\}}
$$

> 这里引入了一个新符号$1\{y^{(i)}=l\}$，这是一个指示函数  
> $1\{true\} = 1$  
> $1\{false\} = 0$

再修改一下，对于训练集中的第i个样本，我们有:
$$
p(y^{(i)}|x^{(i)};\theta) = \prod ^k_{l=1} (\frac {e^{\theta_l^Tx^{(i)}}}{\sum^k_{j=1} e^{\theta_j^T x^{(i)}}})^{1\{y^{(i)}=l\}}, \quad y^{(i)} \in\{1, ..., k\}
$$

### 2. 最大似然估计

于是我们写出 对数似然函数:
$$
\begin{aligned}
    l(\theta)& =\sum^m_{i=1} \log p(y^{(i)}|x^{(i)};\theta)\\
    &= \sum^m_{i=1}log\prod ^k_{l=1}(\frac {e^{\theta_l^Tx^{(i)}}}{\sum^k_{j=1} e^{\theta_j^T x^{(i)}}})^{1\{y^{(i)}=l\}}\\
    &= \sum^m_{i=1} \quad \sum^k_{l=1} 1\{y^{(i)}=l\} \log \frac {e^{\theta_l^Tx^{(i)}}} {\sum^k_{j=1} e^{\theta_j^T x^{(i)}}}\\
    &= \sum^m_{i=1} \quad \sum^k_{l=1} 1\{y^{(i)}=l\} \log p(y^{(i)}=l|x;\theta)
\end{aligned}
$$

所以对数似然函数最大化，就意味着这个式子取最小值:

$$ 
    J(\theta) = - \sum^m_{i=1} \quad \sum^k_{l=1} 1\{y^{(i)}=l\} \log p(y^{(i)}=l|x;\theta)
$$

> 这个式子就是我们常说的交叉熵损失函数$J(\theta) = - \sum\limits^k_{l=1} p_l \log \hat{p_l}$

### 3. 梯度下降法
#### 随机梯度下降
同样先考虑只有一个样本的情况，损失函数就简化下面这样：

$$
\begin{aligned}
    J(\theta) &= - \sum^k_{l=1} 1\{y=l\} \log p(y=l|x;\theta)\\
    &= - \log \frac {e^{\theta_y^T x}} {\sum^k_{j=1} e^{\theta_j^T x}}
\end{aligned}
$$

> 为了后续表述方便，这里记$h_{\theta_i}(x) = \frac {e^{\theta_i^T x}} {\sum^k_{j=1} e^{\theta_j^T x }}$


注意这里的$\theta$是一个矩阵，求偏导时，需要分情况讨论

**1. row == y**:

$$
\begin{aligned}
    \frac  {\partial}{\partial \theta_{(row,j)}} J(\theta) &= - \frac  {\partial}{\partial \theta_{(row,j)}} \log (h_{\theta_y}(x))\\
    &= - \frac 1 {h_{\theta_y}(x)} \cdot \frac {\partial} {\partial \theta_{(row,j)}} h_{\theta_y}(x)\\
    &= - \frac 1 {h_{\theta_y}(x)} \cdot h_{\theta_y}(x) (1- h_{\theta_y}(x)) \cdot \frac {\partial}{\partial \theta_{(row,j)}}\theta^T_{row} x \\
    &= - (1-h_{\theta_y}(x)) \cdot x_j\\
    &= - (1-h_{\theta_{row}}(x)) \cdot x_j
\end{aligned}
$$

**2. row != y**:

$$
\begin{aligned}
    \frac  {\partial}{\partial \theta_{(row,j)}} J(\theta) &= - \frac  {\partial}{\partial \theta_{(row,j)}} \log (h_{\theta_y}(x))\\
    &= - \frac 1 {h_{\theta_y}(x)} \cdot \frac {\partial} {\partial \theta_{(row,j)}} h_{\theta_y}(x)\\
    &= - \frac 1 {h_{\theta_y}(x)} \cdot (- h_{\theta_y}(x) h_{\theta_{row}}(x)) \cdot \frac {\partial}{\partial \theta_{(row,j)}}\theta^T_{row} x \\
    &= h_{\theta_{row}}(x) \cdot x_j\\
    &= - (0-h_{\theta_{row}}(x)) \cdot x_j
\end{aligned}
$$

**汇总**:

在多分类问题中，标签y通常以one-hot的向量形式给出，即$y=(0,0,...,1,...,0)^T$，那么上面两个式子就可以合并:
$$
    \frac  {\partial}{\partial \theta_{(row,j)}} J(\theta) = -(y_{row} - h_{\theta_{row}}(x)) \cdot x_j
$$
> 可以发现，与线性回归、逻辑回归的形式完全一致

于是对单个训练样本，更新规则如下所示：

$$ 
\begin{aligned}
    \theta_{(row,j)} := \theta_{(row,j)} + \alpha (y^{(i)}_{row}-h_{\theta_{row}} (x^{(i)})) x_j^{(i)}
\end{aligned}
$$

#### 批量梯度下降

再考虑有一批样本时，批量梯度下降，就可以写成这样:

$
\begin{aligned}
    &\qquad 重复直到收敛 \{ \\
    &\qquad\qquad\theta_{(row,j)} := \theta_{(row,j)} + \alpha \sum_{i=1}^m (y^{(i)}_{row}-h_{\theta_{row}} (x^{(i)})) x_j^{(i)} \quad(对每个row、j) \\
    &\qquad\}
\end{aligned}
$

## 进一步推广: 广义线性模型
要导出广义线性模型，首先要对我们的模型、给定 $x$ 下 $y$ 的条件分布来做出以下三个假设：

1.	$y | x; \theta ∼ Exponential Family(\eta)$，即给定 $x$ 和 $\theta, y$ 的分布属于指数分布族，是一个参数为 $\eta$ 的指数分布。——假设1
2.	给定 $x$，目的是要预测对应这个给定 $x$ 的 $T(y)$ 的期望值，记为$E[T(y)|x]$。咱们的例子中绝大部分情况都是 $T(y) = y$，这也就意味着我们的学习假设 $h$ 输出的预测值 $h(x)$ 要满足 $h(x) = E[y|x]$。 （注意，这个假设通过对 $h_\theta(x)$ 的选择而满足，在逻辑回归和线性回归中都是如此。）——假设2
> 例如在逻辑回归中， $h_\theta (x) = [p (y = 1|x; \theta)] =[ 0 \cdot p (y = 0|x; \theta)+1\cdot p(y = 1|x;\theta)] = E[y|x;\theta]$
3.	自然参数 $\eta$ 和输入值 $x$ 是线性相关的，$\eta = \theta^T x$，或者如果 $\eta$ 是有值的向量，则有$\eta_i = \theta_i^T x$。——假设3

### 示例1: 线性回归
$y | x; \theta ∼ N(\mu, \sigma^2)$，我们的预测目标$h(x) = E[y|x] = \mu$  
在$N(\mu, \sigma^2) 与 Exponential Family(\eta)$的对应关系中，我们有$\mu = \eta$，于是:  
$h(x) = \mu = \eta = \theta^T x$
### 示例2: 逻辑回归
$y | x; \theta ∼ Bernoulli(\phi)$，我们的预测目标$h(x) = E[y|x] = \phi$  
在$Bernoulli(\phi) 与 Exponential Family(\eta)$的对应关系中，我们有$\phi = \frac 1 1+e^{-\eta}$，于是:  
$h(x) = \phi = \frac 1 {1+e^{-\eta}} = \frac 1 {1+e^{-\theta^T x}} $
### 示例3: softmax回归
$y | x; \theta ∼ Multinoulli(\phi_1,\phi_2,...,\phi_k)$，我们的预测目标$h(x) = E[T(y)|x]$  

**这部分没有理解，后面再补充**