# 决策树与集成学习

## 1. 一些概念
1. 单一决策树  (Decision Tree)
    - 生成算法有**ID3**、**C4.5**、**CART**
    - 完全生长的决策树，容易过拟合 (往往高方差、低偏差)
2. 决策树桩  (Decision Stump)
    - 在训练决策树时，限制树的深度(例如限制为1)，使之成为弱学习模型，这样就成了决策树桩 (往往高偏差、低方差)
3. 集成学习
    1. 袋装法  (Bagging + **Decision Trees**)
        - 训练多棵决策树，最终可降低方差
        - 代表算法: 随机森林
    2. 推进法  (Boosting + **Decision Stumps**)
        - 通过boosting来降低偏差
        - 代表算法: Adaboost、Gradient Boosting

## 2. boosting方法介绍
- 训练N个弱学习模型，最终的预测结果为: f(x) = $\sum\limits_{i=1}^N \alpha_iG_i(x)$
    - 其中$G_i$为第i个模型，$\alpha_i$为第i个模型的权重
- 两种boosting方法:
    1. Adaboost
        - 最流行的boosting算法
        > - for i in range(m):
        >   1. 使用数据集，训练一个弱学习模型$G_i$
        >   2. 计算该模型的权重$\alpha_i$
        >   3. 接下来关注被分错的样本，提高这些样本的权重，生成新的数据集
        > - 最终f(x) = $\sum\limits_{i=1}^N \alpha_iG_i(x)$
        - Adaboost是**正向累加建模**的一个特例
    2. 梯度推进法  (Gradient Boosting)
        - 另外一种代表算法，通过梯度来定位模型的不足
        > - for i in range(m):
        >   1. 使用数据集，训练一个弱学习模型$G_i$
        >   2. 对于每个样本，计算当前模型$G_i$，损失函数的负梯度，即**残差**
        >>  此时，可认为[$G_1$,...,$G_{i-1}$]已固定，为常数，只关注当前模型  
        >>  若损失函数为平方损失，这里负梯度，即残差。若其他损失函数，负梯度为残差的近似值
        >   3. 使用残差作为新的数据集
        > - 最终f(x) = $\sum\limits_{i=1}^N G_i(x)$

## 3. 决策树 与 boosting
1. 梯度提升决策树  GBDT(Gradient Boosting Decision Tree)
    - 还有一些其他名称，例如MART(Multiple Additive Regression Tree)、GBRT(Gradient Boost Regression Tree)等
    - 将决策树桩，作为Gradient Boosting中的弱学习模型，即得到GBDT
2. XGBoost
    - GBDT中，每次训练模型$G_i$，拟合残差时，使用损失函数的一阶导来近似
    - XGBoost中，则使用了损失函数的一阶导、二阶导，训练速度要远远快于传统GBDT
    - 贴两篇参考文章
        1. https://zhuanlan.zhihu.com/p/92837676
        2. xgboost作者的paper https://arxiv.org/pdf/1603.02754.pdf
3. LambdaMART
    > - GBDT算法中，需要计算梯度，对于**point wise**的损失函数，例如平方损失，可以直接求解
    > - 但是排序模型中的常见指标，PNR、NDCG等，无法求梯度
    - 于是诞生了两种新的梯度计算方法:
        1. RankNet
            - 直接比较两条doc的相关性，引入度量公式$P_{ij} = P(doc_i>doc_j) = \frac 1 {1+e^{-\delta(s_i - s_j)}}$ 
            > 这里的$s_i、s_j$为$doc_i、doc_j$对应的相关性打分
            - 再使用交叉熵作为损失函数，就可以计算梯度。优化方向为降低 **pair-wise err**，具体损失函数如下:
            $$
            \begin{aligned}
            L(f(x_i,x_j),y_{i,j})&=-\overline{P}_{ij}\log P_{ij}-(1-\overline{P}_{ij})\log(1-P_{ij})\\
            &=....
            \end{aligned}
            $$
            > 这里的$\overline{P}_{ij}$怎么计算呢?  
            > 对于特定的query，定义$S_{ij} \in \{0,\pm1\}$为$doc_i$和$doc_j$被标记的标签之间的关联，即            
                $$ S_{ij}=\left\{
                \begin{aligned}
                1&&     文档i比文档j更相关\\
                0&&    文档i和文档j相关性一致\\
                -1&&   文档j比文档i更相关
                \end{aligned}
                \right.
                $$
            > 然后用$\overline{P}_{ij}=\frac{1}{2}(1+S_{ij})$表示$doc_i$应该比$doc_j$排序更靠前的已知概率
            - 损失函数就可以进一步去化简
            $$
            \begin{aligned}
            L(f(x_i,x_j),y_{i,j})&=-\overline{P}_{ij}\log P_{ij}-(1-\overline{P}_{ij})\log(1-P_{ij})\\
            &=\frac{1}{2}(1-S_{ij})\sigma(s_i-s_j)+log(1+e^{-\sigma(s_i-s_j)})\\
            &=\begin{cases}
              log\left(1+e^{-\sigma(s_i-s_j)}\right)&&     S_{ij}=1\\
              log\left(1+e^{-\sigma(s_j-s_i)}\right)&&     S_{ij}=-1\\
              \end{cases}
            \end{aligned}
            $$
        2. LambdaRank
            - 通过分析RankNet的梯度，在之基础上，又引入了$\Delta_{NDCG}$，使得优化方向为降低 **list-wise err**
            > 1. 对于RankNet中的损失函数，求解对$s_i、s_j$的偏导
                $$
                \frac{\partial{L}}{\partial{s_i}}=\sigma\left(\frac{1}{2}(1-S_{ij})-\frac{1}{1+e^{\sigma(s_i-s_j)}}\right)=-\frac{\partial{L}}{\partial{s_j}}
                $$
            > 2. 对于每个文档对$(doc_i, doc_j)$，去需要计算损失函数对模型中参数$w_k$的偏导数
                $$
                \frac{\partial{L}}{\partial{w_k}}=\frac{\partial{L}}{\partial{s_i}}\frac{\partial{s_i}}{\partial{w_k}}+\frac{\partial{L}}{\partial{s_j}}\frac{\partial{s_j}}{\partial{w_k}}=\sigma\left(\frac{1}{2}(1-S_{ij})-\frac{1}{1+e^{\sigma(s_i-s_j)}}\right)\left(\frac{\partial{s_i}}{\partial{w_k}}-\frac{\partial{s_j}}{\partial{w_k}}\right)=\lambda_{ij}\left(\frac{\partial{s_i}}{\partial{w_k}}-\frac{\partial{s_j}}{\partial{w_k}}\right)
                $$
            > 其中：
                $$
                \lambda_{ij}=\frac{\partial{L(s_i-s_j)}}{\partial{s_i}}=\sigma\left(\frac{1}{2}(1-S_{ij})-\frac{1}{1+e^{\sigma(s_i-s_j)}}\right)
                $$
            > 3. 这里的$\lambda_{ij}$可以看成是$doc_i$和$doc_j$中间的作用力，如果$doc_i \rhd doc_j$，则$doc_j$会给予$doc_i$向上的大小为$|\lambda_{ij}|$的推动力，而对应地$doc_i$会给予$doc_j$向下的大小为$|\lambda_{ij}|$的推动力
            - 那么如何将NDCG等类似更关注排名靠前的搜索结果的评价指标加入到排序结果之间的推动力中去呢？实验表明，重新定义这样的$\lambda_{ij}$就可以得到很好的效果：
            $$
            \lambda_{ij}=\frac{\partial{L(s_i-s_j)}}{\partial{s_i}}=\frac{-\sigma}{1+e^{\sigma(s_i-s_j)}}|\Delta_{NDCG}|
            $$
            > 其中$|\Delta_{NDCG}|$是交换排序结果$doc_i$和$doc_j$得到的NDCG差值。NDCG倾向于将排名高并且相关性高的文档更快地向上推动，而排名地而且相关性较低的文档较慢地向上推动。  
            > 另外还可以将$|\Delta_{NDCG}|$替换成其他的评价指标。
    - LambdaRank + GBDT(也称为MART)，即LambdaMART