# 感知机
> 在逻辑回归中，我们使用的类别标签 $y \in \{0,1\}$，模型参数$\vec{\theta}$  
> 为了表述方便，在 感知机 及接下来的SVM中  
> 我们修改一下类别标签 $y \in \{-1, +1\}$  
> 模型参数 $\vec{\theta}$，拆分为 $\vec{w}, b$

## 1. 概率解释
> 没有概率解释，无法使用最大似然估计  
> 所以只从损失函数、梯度下降法两方面来介绍

## 2. 损失函数
$J(\vec{w}, b) = - \sum\limits_{x^{(i)} \in 误分类点集合}  y^{(i)} (w^T x^{(i)} + b)$

##### 几何意义
1. $x^{(i)}$被正确分类时，损失为0
2. $y^{(i)} = +1, (w^T x^{(i)} + b) = -1$时，损失为 $-(w^T x^{(i)} + b) = |(w^T x^{(i)} + b)|$
3. $y^{(i)} = -1, (w^T x^{(i)} + b) = +1$时，损失为 $(w^T x^{(i)} + b) = |(w^T x^{(i)} + b)|$

> $|(w^T x^{(i)} + b)|$几何意义为 $x^{(i)}$到分类平面$w^T x + b = 0$的**函数间隔**  
- 所以损失函数可以理解为，所有误分类的点，到分离平面的**函数间隔**之和
- 这里补充一点，感知机是误分类驱动
    - 对于已分类正确的样本，损失函数为0
    - 线性可分数据集，最终损失函数也就为0
    - 因此选择函数间隔、几何间隔均可，感知机使用简单的函数间隔

## 3. 梯度下降法
- $J(w,b)对w的偏导$:

$\quad\quad \frac \partial {\partial w}J(w,b) = - \sum\limits_{x^{(i)} \in 误分类点集合}  y^{(i)} x^{(i)}$

- $J(w,b)对b的偏导$:

$\quad\quad \frac \partial {\partial b}J(w,b) = - \sum\limits_{x^{(i)} \in 误分类点集合}  y^{(i)}$

- 随机梯度下降

于是随机梯度下降看起来就很简洁:  
随机选取一个**误分类点**$(x^{(i)}, y^{(i)})$，更新$w, b$:

$$
    w := w + \alpha y^{(i)} x^{(i)}\\
    b := b + \alpha y^{(i)} 
$$

## 4. 写在最后
> 感知机没什么难点，这里专门总结下，是为了跟后面SVM对比，来加深理解  
