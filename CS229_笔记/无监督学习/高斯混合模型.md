# 高斯混合模型

假设我们得到了某一个训练样本集$\{x^{(1)},...,x^{(m)}\}$  
> 由于这次是非监督学习(unsupervised learning) 环境，所以这些样本就没有什么分类标签了  

## 1. 概率解释
我们希望能够获得一个联合分布 $p(x^{(i)},z^{(i)}) = p(x^{(i)}|z^{(i)})p(z^{(i)})$ 来对数据进行建模  
1. 假设 $z^{(i)} \sim Multinomial(\vec{\phi})$ 
> 即$z^{(i)}$ 是一个以 $\vec{\phi}$ 为参数的多项式分布，假设$z^{(i)}$可能的取值有k个

2. 假设 $x^{(i)}|(z^{(i)}=j) \quad \sim N(μ_j,\Sigma_j)$  
也就是说，我们有k个高斯分布，从$\{1, ..., k\}$中随机选取一个值j，然后假设$x^{(i)}$服从第j个高斯分布。这就是**高斯混合模型**（mixture of Gaussians model） 

> 这与之前的[**高斯判别模型**](CS229_笔记/生成模型/GDA.md)很像:  
$$
\begin{aligned}
    y & \sim Bernoulli(\phi)\\
    x|y = 0 & \sim N(\mu_0,\Sigma)\\
    x|y = 1 & \sim N(\mu_1,\Sigma)\\
\end{aligned}
$$
> 最重要的区别在于[**高斯判别模型**](CS229_笔记/生成模型/GDA.md)中，y是已知的，而这里的z未知

## 2. 最大似然估计
我们写出对数似然函数:  
$$
\begin{aligned}
    l(\phi,\mu,\Sigma) &= \sum_{i=1}^m \log p(x^{(i)};\phi,\mu,\Sigma) \\
    &= \sum_{i=1}^m \log \sum_{z^{(i)}=1}^k p(x^{(i)}|z^{(i)};\mu,\Sigma)p(z^{(i)};\phi)
\end{aligned}
$$
然而，如果我们用设上面方程的导数为零来尝试解各个参数，就会发现根本不可能以闭合形式（closed form）来找到这些参数的最大似然估计（maximum likelihood estimates）  
这就需要用到接下来要介绍的**EM算法**

## 3. EM算法
这里先给出**EM算法**在**高斯混合模型**中，如何应用:

&emsp;重复下列过程直到收敛（convergence）: {

&emsp;&emsp;（$E$-步骤）对每个 $i, j$, 设 

$$
w_j^{(i)} := p(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma)
$$

&emsp;&emsp;（$M$-步骤）最大似然估计，更新参数：

$$
\begin{aligned}
    &\phi_j=\frac 1m\sum_{i=1}^m w_j^{(i)}, \\
    &\mu_j=\frac{\sum_{i=1}^m w_j^{(i)}x^{(i)}}{\sum_{i=1}^m w_j^{(i)}}, \\
    &\Sigma_j=\frac{\sum_{i=1}^m w_j^{(i)}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum_{i=1}^m w_j^{(i)}}.
\end{aligned}
$$

&emsp; } 

## 4. EM vs KMeans
如何理解这里的**EM算法**呢？回想KMeans中，我们的迭代过程:  

&emsp; 重复直到收敛 {

&emsp;&emsp; 将每个训练样本$x^{(i)}$ “分配”给距离最近的**聚类重心**$\mu_j$

&emsp;&emsp; 重新调整每个聚类重心$\mu_j$

&emsp; }

**EM算法**其实很类似：

&emsp; 重复直到收敛 {

&emsp;&emsp; ($E$-步骤): &emsp; 基于目前的参数，计算每个训练样本$x^{(i)}$属于第j个高斯分布的概率

> &emsp;&emsp; 类似于上面，每个训练样本$x^{(i)}$“分配”给概率最大的高斯分布

&emsp;&emsp; ($M$-步骤): &emsp; 更新参数，即重新调整每个高斯分布

&emsp; }