# 朴素贝叶斯

## 1. 概率解释
在GDA中，特征X为连续值，我们做了这样的假设:  
$$
\begin{aligned}
    y & \sim Bernoulli(\phi)\\
    x|y = 0 & \sim N(\mu_o,\Sigma)\\
    x|y = 1 & \sim N(\mu_1,\Sigma)\\
\end{aligned}
$$

**朴素贝叶斯**，是特征X为离散值时，一种学习算法  
**1.1 首先做一个非常强的假设，特征X的每一维之间，是相互独立的**:  
$$
\begin{aligned}
    p(x_1, ..., x_n|y) & = p(x_1|y)p(x_2|y,x_1)p(x_3|y,x_1,x_2) ... p(x_n|y,x_1,x_2,...,x_{n-1}) \\
    & = p(x_1|y)p(x_2|y)p(x_3|y) ... p(x_n|y) \\
    & = \prod^n_{j=1}p(x_j|y) \\
\end{aligned}
$$
**1.2 然后对于每一个$p(x_j|y)$，假设$x|y \sim Bernoulli(\phi)$**  
> 这里上课时，以`自动识别垃圾邮件`为例  
> 每一维特征$x_j$，表示字典中第j个词是否出现，因此$x_j \in (0,1)$

$$
\begin{aligned}
    x|y = 0 & \sim Bernoulli(\phi_{j|y=0})\\
    x|y = 1 & \sim Bernoulli(\phi_{j|y=1})\\
\end{aligned}
$$

## 2. 最大似然估计

似然函数:  
$$
\mathcal{L}(\phi_y,\phi_{j|y=0},\phi_{j|y=1})=\prod^m_{i=1}p(x^{(i)},y^{(i)})
$$

找到使联合似然函数取得最大值的对应参数组合 $\phi_y , \phi_{i|y=0} 和 \phi_{i|y=1}$ 就给出了最大似然估计：

$$
\begin{aligned}
    \phi_{y} &= \frac{\sum^m_{i=1}1\{y^{(i)} =1\}}{m}\\
    \phi_{j|y=1} &=\frac{\sum^m_{i=1}1\{x_j^{(i)} =1 \wedge y^{(i)} =1\} }{\sum^m_{i=1}1\{y^{(i)} =1\}} \\
    \phi_{j|y=0} &= \frac{\sum^m_{i=1}1\{x_j^{(i)} =1 \wedge y^{(i)} =0\} }{\sum^m_{i=1}1\{y^{(i)} =0\}} \\
\end{aligned}
$$
> 在上面的等式中，"$\wedge$(and)"这个符号的意思是逻辑"和"

## 3. 推广
朴素贝叶斯，也可以处理`多分类`问题  
每一维特征$x_j$，也不必局限于$x_j \in (0,1)$，可以有多个取值  
此时的**朴素贝叶斯**:
$$
\begin{aligned}
    p(y=k) &= \frac {\sum^m_{i=1} 1\{y^{(i)} =k\}} {m}\\
    p(x_j=a_{jl}|y=k) &= \frac {\sum^m_{i=1} 1\{x_j^{(i)} =a_{jl} \wedge y^{(i)} =k\} } {\sum^m_{i=1} 1\{y^{(i)} =k\}} \\
\end{aligned}
$$
> X的第j维特征$x_j$可能有L个取值  
> $a_{jl}$ 表示$x_j$的 第l个取值

## 4. 拉普拉斯平滑
在计算概率$p$时，可能会出现概率值为0的情况  
为避免这样的情况，引入了拉普拉斯平滑:  
$$
\begin{aligned}
    p(y=k) &= \frac {\sum^m_{i=1} 1\{y^{(i)} =k\} + 1} {m+K}\\
    p(x_j=a_{jl}|y=k) &= \frac {\sum^m_{i=1} 1\{x_j^{(i)} =a_{jl} \wedge y^{(i)} =k\} + 1} {\sum^m_{i=1} 1\{y^{(i)} =k\} + L} \\
\end{aligned}
$$

