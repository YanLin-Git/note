# 几种激活函数

## 一、阶跃函数
$$
h(x) =  \begin{cases}
        0&&     x<0\\
        1&&     x>=0\\
        \end{cases}
$$
- 代码实现
    ```
    import numpy as np
    def step_function(x):
        '''
        例如 x = [-1, 1, 2]
        '''
        y = x > 0 # y=[False, True, True]
        return y.astype(np.int) # y=[0,1,1]
    ```

## 二、 sigmoid函数
$$
h(x) = \frac 1 {1+e^{-x}}
$$
- 代码实现
    ```
    import numpy as np
    def sigmoid(x):
        return 1 / (1 + np.exp(-x))
    ```
- 权重初始化，Xavier初始值
    - 标准差为$\frac 1 {\sqrt n}$的分布, `w=np.random.randn(*input.shape) / np.sqrt(node_num)`

## 三、ReLU(Rectified Linear Unit)
$$
h(x) =  \begin{cases}
        0&&     x<=0\\
        x&&     x>0\\
        \end{cases}
$$
- 代码实现
    ```
    import numpy as np
    def relu(x):
        return np.maximum(0,x) #输出0和x中的较大值
    ```
- 权重初始化，He初始值
    - 标准差为$\frac 2 {\sqrt n}$的分布, `w=np.random.randn(*input.shape) / np.sqrt(node_num) * 2`

## 四、tanh函数

## 五、gelu
- bert中的前馈神经网络、预测mask掉的单词时，使用的激活函数
- 代码实现
    ```
    def gelu(x):
        return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
    ```