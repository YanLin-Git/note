# 排序模型简单总结
## 一、排序模型中的常用指标
1. **PNR**  (Positive Negative Rate)
    > 1. 假设有10条doc, 可组成$C_{10}^{2}$=45对pair
    > 2. 统计其中对正逆序pair个数
    >     - 例如正序对: 30, 逆序对: 15
    > 3. 则pnr = $\frac {正序对数} {逆序对数}$ = $\frac {30} {15}$ = 2
2. **NDCG**  (Normalized Discounted Cumulative Gain)
- 假设一个query下，召回n条doc:
|index|相关性打分|权重 = $\frac 1 {\log_2 {(i+1)}}$|相关性打分*权重|
|---|---|---|---|
|1|3|$\frac 1 {\log_2 {(1+1)}}$ = 1|3*1|
|2|2|$\frac 1 {\log_2 {(2+1)}}$ = 0.63|2*0.63|
|3|3|$\frac 1 {\log_2 {(3+1)}}$ = 0.5|3*0.5|
|4|0|$\frac 1 {\log_2 {(4+1)}}$ = 0.43|0*0.43|
|5|1|$\frac 1 {\log_2 {(5+1)}}$ = 0.39|1*0.39|
|...||排序靠后的doc，权重会逐渐降低||
> 1. $CG_k = \sum\limits_{i=1}^k {相关性打分_i}$
> 2. $DCG_k = \sum\limits_{i=1}^k {相关性打分_i} * {权重_i} = \sum\limits_{i=1}^k {\frac {相关性打分_i}  {\log_2 {(i+1)}}}$
> 3. $IDCG_k$ = 理想排序情况下的$DCG_k$
>     - 例如上表中，理想排序可能为 [doc1, doc3, **doc8**, doc2, **doc7**, doc5, doc4, ...]，再重新计算$DCG_k$
> 4. $NDCG_k = \frac {DCG_k} {IDCG_k}$

## 二、三种损失函数
1. point wise
    - 预测doc得分，损失函数: $L(f(x_i), y_i)$
2. pair wise
    - 预测两条doc之间的正逆序，损失函数: $L(f(x_i,x_j),y_{i,j})$，可优化**PNR**这样的指标
3. list wise
    - 可优化**NDCG**这样的指标

## 三、决策树 与 集成学习 的一些概念
1. 单一决策树  (Decision Tree)
    - 完全生长的决策树，容易过拟合 (往往高方差、低偏差)
2. 决策树桩  (Decision Stump)
    - 在训练决策树时，限制树的深度(例如限制为1)，使之成为弱学习模型，这样就成了决策树桩 (往往高偏差、低方差)
3. 集成学习
    1. 袋装法  (Bagging + **Decision Trees**)
        - 训练多棵决策树，最终可降低方差
        - 代表算法: 随机森林
    2. 推进法  (Boosting + **Decision Stumps**)
        - 后续会重点介绍这种方法，通过boosting来降低偏差
        - 代表算法: Adaboost、GBDT、XGBoost

## 四、boosting方法介绍
- 训练N个弱学习模型，最终的预测结果为: f(x) = $\sum\limits_{i=1}^N \alpha_iG_i(x)$
    - 其中$G_i$为第i个模型，$\alpha_i$为第i个模型的权重
- 几种boosting方法:
    1. Adaboost
        - 最流行的boosting算法
        > - for i in range(m):
        >   1. 使用数据集，训练一个弱学习模型$G_i$
        >   2. 计算该模型的权重$\alpha_i$
        >   3. 接下来关注被分错的样本，提高这些样本的权重，生成新的数据集
        > - 最终f(x) = $\sum\limits_{i=1}^N \alpha_iG_i(x)$
    2. 正向累加建模  (Forward Stagewise Additive Modeling)
        - Adaboost只是其中的一个特例
    3. 梯度推进法  (Gradient Boosting)
        - 另外一种代表算法，通过梯度来定位模型的不足，排序模型中常使用这种
        > - for i in range(m):
        >   1. 使用数据集，训练一个弱学习模型$G_i$
        >   2. 对于每个样本，计算当前模型$G_i$，损失函数的负梯度，即**残差**
        >       - 此时，可认为[$G_1$,...,$G_{i-1}$]已固定，为常数，只关注当前模型
        >       - 若损失函数为平方损失，这里负梯度，即残差。若其他损失函数，负梯度为残差的近似值
        >   3. 使用残差作为新的数据集
        > - 最终f(x) = $\sum\limits_{i=1}^N G_i(x)$

## 五、决策树 与 boosting
1. 梯度提升决策树  GBDT(Gradient Boosting Decision Tree)
    - 还有一些其他名称，例如MART(Multiple Additive Regression Tree)、GBRT(Gradient Boost Regression Tree)等
    - 将决策树桩，作为Gradient Boosting中的弱学习模型，即得到GBDT
2. LambdaMART
    > - GBDT算法中，需要计算梯度，对于**point wise**的损失函数，例如平方损失，可以直接求解
    > - 但是排序模型中的常见指标，PNR、NDCG等，无法求梯度
    - 于是诞生了两种新的梯度计算方法:
        1. RankNet
            - 直接比较两条doc的相关性，引入度量公式$P_{ij} = P(doc_i>doc_j) = \frac 1 {1+e^{-\delta(s_i - s_j)}}$ 
            > 这里的$s_i、s_j$为$doc_i、doc_j$对应的相关性打分
            - 再使用交叉熵作为损失函数，就可以计算梯度。优化方向为降低 **pair-wise err**，具体损失函数如下:
            $$
            \begin{aligned}
            L(f(x_i,x_j),y_{i,j})&=-\overline{P}_{ij}\log P_{ij}-(1-\overline{P}_{ij})\log(1-P_{ij})\\
            &=....
            \end{aligned}
            $$
            > 这里的$\overline{P}_{ij}$怎么计算呢?  
            > 对于特定的query，定义$S_{ij} \in \{0,\pm1\}$为$doc_i$和$doc_j$被标记的标签之间的关联，即            
                $$ S_{ij}=\left\{
                \begin{aligned}
                1&&     文档i比文档j更相关\\
                0&&    文档i和文档j相关性一致\\
                -1&&   文档j比文档i更相关
                \end{aligned}
                \right.
                $$
            > 然后用$\overline{P}_{ij}=\frac{1}{2}(1+S_{ij})$表示$doc_i$应该比$doc_j$排序更靠前的已知概率
            - 损失函数就可以进一步去化简
            $$
            \begin{aligned}
            L(f(x_i,x_j),y_{i,j})&=-\overline{P}_{ij}\log P_{ij}-(1-\overline{P}_{ij})\log(1-P_{ij})\\
            &=\frac{1}{2}(1-S_{ij})\sigma(s_i-s_j)+log(1+e^{-\sigma(s_i-s_j)})\\
            &=\begin{cases}
              log\left(1+e^{-\sigma(s_i-s_j)}\right)&&     S_{ij}=1\\
              log\left(1+e^{-\sigma(s_j-s_i)}\right)&&     S_{ij}=-1\\
              \end{cases}
            \end{aligned}
            $$
        2. LambdaRank
            - 通过分析RankNet的梯度，在之基础上，又引入了$\Delta_{NDCG}$，使得优化方向为降低 **list-wise err**
            > 1. 对于RankNet中的损失函数，求解对$s_i、s_j$的偏导
                $$
                \frac{\partial{L}}{\partial{s_i}}=\sigma\left(\frac{1}{2}(1-S_{ij})-\frac{1}{1+e^{\sigma(s_i-s_j)}}\right)=-\frac{\partial{L}}{\partial{s_j}}
                $$
            > 2. 对于每个文档对$(doc_i, doc_j)$，去需要计算损失函数对模型中参数$w_k$的偏导数
                $$
                \frac{\partial{L}}{\partial{w_k}}=\frac{\partial{L}}{\partial{s_i}}\frac{\partial{s_i}}{\partial{w_k}}+\frac{\partial{L}}{\partial{s_j}}\frac{\partial{s_j}}{\partial{w_k}}=\sigma\left(\frac{1}{2}(1-S_{ij})-\frac{1}{1+e^{\sigma(s_i-s_j)}}\right)\left(\frac{\partial{s_i}}{\partial{w_k}}-\frac{\partial{s_j}}{\partial{w_k}}\right)=\lambda_{ij}\left(\frac{\partial{s_i}}{\partial{w_k}}-\frac{\partial{s_j}}{\partial{w_k}}\right)
                $$
            > 其中：
                $$
                \lambda_{ij}=\frac{\partial{L(s_i-s_j)}}{\partial{s_i}}=\sigma\left(\frac{1}{2}(1-S_{ij})-\frac{1}{1+e^{\sigma(s_i-s_j)}}\right)
                $$
            > 3. 这里的$\lambda_{ij}$可以看成是$doc_i$和$doc_j$中间的作用力，如果$doc_i \rhd doc_j$，则$doc_j$会给予$doc_i$向上的大小为$|\lambda_{ij}|$的推动力，而对应地$doc_i$会给予$doc_j$向下的大小为$|\lambda_{ij}|$的推动力
            - 那么如何将NDCG等类似更关注排名靠前的搜索结果的评价指标加入到排序结果之间的推动力中去呢？实验表明，重新定义这样的$\lambda_{ij}$就可以得到很好的效果：
            $$
            \lambda_{ij}=\frac{\partial{L(s_i-s_j)}}{\partial{s_i}}=\frac{-\sigma}{1+e^{\sigma(s_i-s_j)}}|\Delta_{NDCG}|
            $$
            > 其中$|\Delta_{NDCG}|$是交换排序结果$doc_i$和$doc_j$得到的NDCG差值。NDCG倾向于将排名高并且相关性高的文档更快地向上推动，而排名地而且相关性较低的文档较慢地向上推动。  
            > 另外还可以将$|\Delta_{NDCG}|$替换成其他的评价指标。
    - LambdaRank + GBDT(也称为MART)，即LambdaMART
3. GBRank
    > - 需要阅读c++源码去理解模型结构，未开源，感觉以后是用不到了，不去浪费时间..
    > - 相比LambdaMART、或者XGBoost，优势在于增加了`loss/fix机制`，可以调整数据集中，特定样本的权重
    > - 看名字，猜测是GBDT的一个变种
4. XGBoost
    - GBDT中，每次训练模型$G_i$，拟合的是残差，使用损失函数的一阶导来近似
    - XGBoost中，则使用了损失函数的一阶导、二阶导，训练速度要远远快于传统GBDT
    - 贴两篇参考文章
        1. https://zhuanlan.zhihu.com/p/92837676
        2. xgboost作者的paper https://arxiv.org/pdf/1603.02754.pdf

## 六、其他
1. LambdaMART
    - 开源工具包Ranklib
    - 没研究怎么使用，2021年搜狗系统中，综搜会使用，2022已转GBRank
    - 参考文献: [网上随便搜的，没找到作者，还需要自己转格式，就放在本地了](note/排序算法_参考文献.md)

2. GBRank
    - 损失函数可选择`pairwise`or`pointwise`
    - 例如精排模块中，视频相关性打分预测，采用: `(1-w)*pairwise + w*pointwise`
3. XGBoost
    - XGBoost库的使用
    ```
    import xgboost as xgb
    import numpy as np
    dataTrain = xgb.DMatrix(features, label=labels) #将数据集构造为xgboost中可以用的格式
    params={'booster':'gbtree', #基于树模型
            'nthread':12, #线程数
            'objective': 'rank:pairwise', #损失函数
            'eval_metric':'auc',
            'seed':0,
            'eta': 0.01,
            'gamma':0.1,
            'min_child_weight':1.1,
            'max_depth':5,
            'lambda':10,
            'subsample':0.7,
            'colsample_bytree':0.7,
            'colsample_bylevel':0.7
            'tree_method':'exact'
            }
    watchlist = [(dataTrain,'train')]
    model = xgb.train(params,dataTrain,num_boost_round=1000,evals=watchlist)
    ```
    - 重点关注这里的objective
        1. 二分类问题: `binary:logistic`
        2. 多分类问题: `multi:softmax`
        3. 排序问题: `rank:pairwise`
        4. 也可自定义，例如精排模块中，视频相关性打分预测，采用: `(1-w)*pairwise + w*pointwise`

