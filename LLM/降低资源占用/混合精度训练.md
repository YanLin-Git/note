# 混合精度训练

- paper: https://arxiv.org/pdf/1710.03740.pdf

## 一、混合精度
> 部分运算操作使用`FP16`，从而加速计算  
> 部分运算操作使用`FP32`，避免舍入误差  
- 具体混和方式，可分为4个等级
    1. **O0**: 全部使用`FP32`
    2. **O1(推荐使用)**: 根据黑白名单，自动决定哪些参数使用`FP32`，哪些参数使用`FP16`
    3. **O2**: batch norm使用`FP32`，其它使用`FP16`
    4. **O3**: 全部使用`FP16`

## 二、损失缩放(loss scaling)
- 如下方表格所示，在一个标准训练流程中，添加了`3`、`5`两步
    ||阶段|精度|备注|
    |---|---|---|---|
    |1|前向传播|FP16|output = model(input)|
    |2|计算loss|FP32|loss = loss_fn(output, target)|
    |3|`scale`|FP32--->FP16|loss是`FP32`，可能不在`FP16`的表示范围内，因此需要进行缩放(扩大一定倍数)|
    |4|后向传播|FP16|loss.backward()|
    |5|`unscale`|FP16--->FP32|梯度是`FP16`，需要unscale回去，转化为`FP32`|
    |6|参数更新|FP32|optimizer.step()|
    |7|梯度清零||optimizer.zero_grad()|

## 三、在代码中使用
<details>
<summary><b>1、标准训练流程</b></summary>

```python
output = model(input)           # 前向传播
loss = loss_fn(output, target)  # 计算损失
loss.backward()                 # 后向传播
optimizer.step()                # 参数更新
optimizer.zero_grad()           # 梯度清零
```

</details>


<details>
<summary><b>2、使用apex实现混合精度训练</b></summary>

```python
from apex import amp

# 封装一下model、optimizer，这里指定混合精度的等级为"O1"
model, optimizer = amp.initialize(model, optimizer, opt_level='O1')

output = model(input)           # 前向传播
loss = loss_fn(output, target)  # 计算损失

with amp.scale_loss(loss, optimizer) as scaled_loss:
    scaled_loss.backward()      # 损失缩放、后向传播

optimizer.step()                # 参数更新
optimizer.zero_grad()           # 梯度清零
```

</details>


<details>
<summary><b>3、使用pytorch实现</b></summary>

```python

scaler = torch.cuda.amp.GradScaler()

with torch.cuda.amp.autocast():
    output = model(input)           # 前向传播
    loss = loss_fn(output, target)  # 计算损失

scaler.scale(loss).backward()       # 损失缩放、后向传播
scaler.step(optimizer)              # 参数更新
scaler.update()

optimizer.zero_grad()               # 梯度清零
```

</details>