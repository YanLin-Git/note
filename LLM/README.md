# LLM(大语言模型)

- 目录

    1. [如何降低资源占用？](LLM/降低资源占用/)
        - 随着模型规模扩大，需要寻找各种方法来减少资源需求
    2. [并行训练](LLM/并行训练/)
    3. [推理加速](LLM/推理加速/)
    4. [训练流程](LLM/训练流程/)
        - 大模型的训练阶段、训练方式、训练算法
    5. [LLM的应用](LLM/LLM应用/)

- 模型汇总

    |模型|发布方|参数量|备注|
    |---|---|---|---|
    |LLaMA|Meta|65B|[paper](https://arxiv.org/abs/2302.13971v1)|
    |LLaMA2|Meta||[paper](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)|
    |Alpaca|斯坦福|7B|效果可媲美175B的LLM|
    |vicuna||||
    |Chinese-LLaMA-Alpaca|哈工大||https://github.com/ymcui/Chinese-LLaMA-Alpaca|
    |GLM|清华||https://github.com/THUDM/GLM|
    |ChatGLM-6B|清华||https://github.com/THUDM/ChatGLM-6B|
    |ChatGLM2-6B|清华||https://github.com/THUDM/ChatGLM2-6B|
    |GPT-4|openAI|||