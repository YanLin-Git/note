# 常见正则方法
> 为了防止过拟合

## 1. 权重衰减
> 即L2正则

## 2. dropout
- 训练时，随机丢弃隐藏层的一些神经元，使得训练时不会过度依赖某些特定的神经元
- 测试时，一般不使用dropout
- 代码实现:
```python
def dropout(X, drop_prob):
    X = X.float()
    assert 0 <= drop_prob <= 1
    keep_prob = 1 - drop_prob
    
    # 这种情况下把全部元素都丢弃
    if keep_prob == 0:
        return torch.zeros_like(X)
    
    mask = (torch.rand(X.shape) < keep_prob).float()

    return mask * X / keep_prob # 注意这里最后 '/keep_prob'
```

## 3. normalize
> - 导读: https://zhuanlan.zhihu.com/p/38176412
> - 各种normalize方法: https://zhuanlan.zhihu.com/p/43200897

|normalization|备注|适用场景|
|---|---|---|
|batch normalization|对于同一个神经元，mini-batch中不同训练实例导致的不同激活|优先考虑|
|layer normalization|同隐层的所有神经元|对于RNN，目前只有Layer有效|
|instance normalization|CNN中卷积层的单个通道|GAN等图片生成类任务|
|group normalization|layer和instance的折衷||