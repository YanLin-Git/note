# 学习率调整 (lr_scheduler)

## 1. 随步数衰减
- 每经过几个周期，就根据一些因素降低学习率
    > 例如训练过程中，观察验证集的错误率，当错误率停止下降，就降低学习率
- pytorch实现: `torch.optim.lr_scheduler.ReduceLROnPlateau`

## 2. 指数衰减
- $ \eta_t = \eta_0 e^{-kt} = \eta_0 \gamma^t$
    > $\eta_0, k$是超参数，t是迭代次数
    > pytorch的实现中，令$e^{-k} = \gamma$ ，直接指定$\gamma$参数，例如 $\gamma$=0.95
- pytorch实现: `torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma)`

## 3. 1/t衰减
- $ \eta_t = \frac {\eta_0} {1+kt} $
    > $\eta_0, k$是超参数，t是迭代次数

## 5. CosineAnnealingLR
- $ \eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})\left(1 + \cos\left(\frac{T_{cur}}{T_{max}}\pi\right)\right) $

## 6. pytorch中更多实现
||paper|
|---|---|
|CosineAnnealingLR|https://arxiv.org/abs/1608.03983|
|CosineAnnealingWarmRestarts|https://arxiv.org/abs/1608.03983|
|CyclicLR|https://arxiv.org/abs/1506.01186|
|OneCycleLR|https://arxiv.org/abs/1708.07120|
|LambdaLR|自定义lambda函数，根据epoch来计算相应的学习率|
