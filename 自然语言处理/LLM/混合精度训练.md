# 混合精度训练

- paper: https://arxiv.org/pdf/1710.03740.pdf

## 一、混合精度
> 部分运算操作使用`FP16`，从而加速计算  
> 部分运算操作使用`FP32`，避免舍入误差  
- 具体混和方式，可分为4个等级
    1. **O0**: 全部使用`FP32`
    2. **O1**: 根据黑白名单，自动决定哪些参数使用`FP32`，哪些参数使用`FP16`
    3. **O2**: batch norm使用`FP32`，其它使用`FP16`
    4. **O3**: 全部使用`FP16`

## 二、损失缩放(loss scaling)
1. 前向传播时，使用`FP16`，计算loss
    > 这里得到的loss是`FP32`，可能不在`FP16`的表示范围内，因此需要进行缩放(**scale**)
2. 使用缩放后的loss，反向传播，得到`FP16`的梯度
    > 再将**scale**后的梯度**unscale**回去，转化为`FP32`
3. 更新参数

## 三、在代码中使用
<details>
<summary><b>1、标准训练流程</b></summary>

```python
output = model(input)           # 前向传播
loss = loss_fn(output, target)  # 计算损失
loss.backward()                 # 后向传播
optimizer.step()                # 参数更新
optimizer.zero_grad()           # 梯度清零
```

</details>


<details>
<summary><b>2、使用apex实现混合精度训练</b></summary>

```python
from apex import amp

# 封装一下model、optimizer，这里指定混合精度的等级为"O1"
model, optimizer = amp.initialize(model, optimizer, opt_level='O1')

output = model(input)           # 前向传播
loss = loss_fn(output, target)  # 计算损失

with amp.scale_loss(loss, optimizer) as scaled_loss:
    scaled_loss.backward()      # 损失缩放、后向传播

optimizer.step()                # 参数更新
optimizer.zero_grad()           # 梯度清零
```

</details>


<details>
<summary><b>3、使用pytorch实现</b></summary>

```python

scaler = torch.cuda.amp.GradScaler()

with torch.cuda.amp.autocast():
    output = model(input)           # 前向传播
    loss = loss_fn(output, target)  # 计算损失

scaler.scale(loss).backward()       # 损失缩放、后向传播
scaler.step(optimizer)              # 参数更新
scaler.update()

optimizer.zero_grad()               # 梯度清零
```

</details>