# 优化算法


## 1. 梯度下降
- 梯度下降
- 随机梯度下降
- 批梯度下降
$$
    \bm{\theta}_t = \bm{\theta}_{t-1} - \eta_t \bm{g}_t
$$
> 这里的$\eta_t$可以随着时间，自我衰减，例如:  
> $\eta_t = \eta t^\alpha, \quad \alpha = -1, -0.5$  
> $\eta_t = \eta \alpha^t, \quad \alpha = 0.95$

- 代码实现:
```python
# pytorch中指定一个批梯度下降算法:

import torch.optim as optim
optimizer = optim.SGD(net.parameters(), lr=0.03) # 学习率0.03
```

## 2. 牛顿法
- 实例: 最大熵模型、CRF

## 3. 坐标下降法
- 实例: SVM中的SMO算法

## 4. Momentum
#### 引入一个新的变量 $\bm{v}_t$，初始化 $\bm{v}_0=0$
$$
    \bm{v}_t = \gamma \bm{v}_{t-1} + \eta_t \bm{g}_t \\
    \bm{\theta}_t = \bm{\theta}_{t-1} - \bm{v}_t
$$
> 其中 $0 \le \gamma < 1$  
- 这里的 $\bm{v}_t$，相当于最近$\frac 1 {1-\gamma}$个时间步的$\bm{g}_t$的加权和  
    > 例如$\gamma=0.5$，$\bm{v}_t$相当于最近2个时间步的`小批量梯度`的加权和  
    > 例如$\gamma=0.9$，$\bm{v}_t$相当于最近10个时间步的`小批量梯度`的加权和

- 代码实现:
```python
# 相比SGD，只需要多指定一个 参数momentum:

import torch.optim as optim
optimizer = optim.SGD(net.parameters(), lr=0.03, momentum=0.9) # 学习率0.03, gamma=0.9
```

## 5. AdaGrad(Adaptive Gradient)
> 在之前的算法中，$\vec{\theta} = (\theta_1,\theta_2,\cdots)$中的每个$\theta_i$按照同一个学习率$\eta$来更新  
> **AdaGrad**根据每个维度梯度值的大小，来调整每个维度的学习率

#### 引入一个新的变量 $\bm{s}_t$，初始化 $\bm{s}_0=0$
$$
    \bm{s}_t = \bm{s}_{t-1} + \bm{g}_t \odot \bm{g}_t
$$
> $\odot$表示逐元素乘积， $\bm{s}_t$ 保存了以前所有梯度值的平方和

#### 然后更新参数:  
$$
    \bm{\theta}_t = \bm{\theta}_{t-1} - \eta \frac 1 {\sqrt{\bm{s}_t + \epsilon}} \odot \bm{g}_t
$$
> $\epsilon$ 是为了维持数值稳定性而添加的常数，如$10^{-6}$  
> 更新参数时，根据 $\bm{s}_t$ 调整学习尺度。意味着变动较大的参数，学习率逐渐变小

- 代码实现:
```python
# pytorch中使用 AdaGrad

import torch.optim as optim
optimizer = optim.Adagrad(net.parameters(), lr=0.1) # 学习率0.1
```

## 6. RMSProp
> AdaGrad会记录过去所有梯度的平方和，$\bm{s}_t$一直在累加。学习越深入，更新幅度越小  
#### RMSProp做了进一步修改，会逐渐遗忘过去的梯度，更多地关注 较新的梯度：

$$
    \bm{s}_t = \gamma \bm{s}_{t-1} + (1-\gamma) \bm{g}_t \odot \bm{g}_t
$$
> 类似于**Momentum**，$\bm{s}_t$相当于最近$\frac 1 {1-\gamma}$个时间步的 $\bm{g}_t \odot \bm{g}_t$ 的加权平均

#### 然后再按照这样的方式去更新参数:  
$$
    \bm{\theta}_t = \bm{\theta}_{t-1} - \eta \frac 1 {\sqrt{\bm{s}_t + \epsilon}} \odot \bm{g}_t
$$

- 代码实现:
```python
# 相比AdaGrad，多指定一个参数alpha

import torch.optim as optim
optimizer = optim.RMSprop(net.parameters(), lr=0.1, alpha=0.9) # 学习率0.1, gamma=0.9
```

## 7. AdaDelta
- 与**RMSProp**类似，也是针对**AdaGrad**做了修改，这里略...

## 8. Adam
> Momentum 与 RMSProp的结合

#### 首先，类似Momentum中那样更新 $\bm{v}_t$
$$
    \bm{v}_t = \beta_1 \bm{v}_{t-1} + (1-\beta_1) \bm{g}_t 
$$

#### 然后，类似RMSProp中那样更新 $\bm{s}_t$
$$
    \bm{s}_t = \beta_2 \bm{s}_{t-1} + (1-\beta_2) \bm{g}_t \odot \bm{g}_t
$$

> 这里有个问题就是t较小时，相应的$\bm{v}_t、\bm{s}_t$也比较小  
> 例如，取$\beta_1=0.9$时，$\bm{v}_1 = 0.9 * \bm{v}_0 + 0.1 * \bm{g}_1 = 0.1 \bm{g}_1$

#### 于是，Adam中对$\bm{v}_t、\bm{s}_t$做了进一步修正
$$
    \hat{\bm{v}}_t = \frac {\bm{v}_t} {1-(\beta_1)^t} \\
    \hat{\bm{s}}_t = \frac {\bm{s}_t} {1-(\beta_2)^t} \\
$$

#### 最后，更新参数
$$
    \bm{\theta}_t = \bm{\theta}_{t-1} - \eta \frac 1 {\sqrt{\bm{s}_t + \epsilon}} \odot \bm{v}_t
$$

- 代码实现:
```python
# pytorch中使用 Adam

import torch.optim as optim
optimizer = optim.Adam(net.parameters(), lr=0.1) # 学习率0.1
```