# 几种激活函数
> 参考: https://baijiahao.baidu.com/s?id=1653421414340022957&wfr=spider&for=pc
> https://zhuanlan.zhihu.com/p/380637005?ivk_sa=1024320u

## 1. 阶跃函数
$$
    h(x) =  \begin{cases}
            0 & \quad x < 0 \\
            1 & \quad x \ge 0 \\
            \end{cases}
$$
- 代码实现
    ```
    import numpy as np
    def step_function(x):
        '''
        例如 x = [-1, 1, 2]
        '''
        y = x > 0 # y=[False, True, True]
        return y.astype(np.int) # y=[0,1,1]
    ```

## 2. sigmoid函数
$$
    sigmoid(x) = \sigma = \frac 1 {1+e^{-x}} \\
    sigmoid'(x) = \sigma (1-\sigma)
$$
- 代码实现
    ```
    import numpy as np
    def sigmoid(x):
        return 1 / (1 + np.exp(-x))
    ```
- 权重初始化，Xavier初始值
    - 标准差为$\frac 1 {\sqrt n}$的高斯分布, `w=np.random.randn(*input.shape) / np.sqrt(node_num)`

## 3. tanh函数
> **sigmoid**的值域为(0,1)，**tanh**形状与**sigmoid**很像，值域为(-1,1)
$$
    tanh(x) = \frac {e^x - e^{-x}} {e^x + e^{-x}} \\
    tanh'(x) = 1 - tanh^2(x)
$$
- 代码实现
    ```
    import numpy as np
    def tanh(x):
        return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))
    ```
- 权重初始化，与`sigmoid`相同，Xavier初始值
    - 标准差为$\frac 1 {\sqrt n}$的高斯分布, `w=np.random.randn(*input.shape) / np.sqrt(node_num)`

## 4. ReLU(Rectified Linear Unit)
> 相比**sigmoid**，能避免`梯度消失`问题  
> 但是引入了`死亡ReLU`问题，即网络的大部分分量都不会更新

$$
    ReLU(x) =
    \begin{cases}
        0 & \quad x \le 0 \\
        x & \quad x > 0 \\
    \end{cases} \\
    ReLU'(x) =
    \begin{cases}
        0 & \quad x \le 0 \\
        1 & \quad x > 0 \\
    \end{cases} \\
$$
- 代码实现
    ```
    import numpy as np
    def relu(x):
        return np.maximum(0,x) #输出0和x中的较大值
    ```
- 权重初始化，He初始值
    - 标准差为$\frac 2 {\sqrt n}$的高斯分布, `w=np.random.randn(*input.shape) / np.sqrt(node_num) * 2`

## 5. ELU(指数线性单元)
> 相比**ReLU**，能避免`死亡ReLU`问题  
> 但是包含指数运算，计算时间更长

$$
    ELU(x) =
    \begin{cases}
        \alpha(e^x - 1) & \quad x \le 0 \\
        x & \quad x > 0 \\
    \end{cases} \\
    ELU'(x) =
    \begin{cases}
        \alpha e^x = ELU(x) + \alpha & \quad x \le 0 \\
        1 & \quad x > 0 \\
    \end{cases} \\
$$
> 通常 $\alpha \in (0.1, 0.3)$

## 6. Leaky ReLU(渗漏型整流线性单元)
> 能避免`死亡ReLU`问题  
> 相比**ELU**，不包含指数运算，计算时间更快

$$
    LReLU(x) =
    \begin{cases}
        \alpha x & \quad x \le 0 \\
        x & \quad x > 0 \\
    \end{cases} \\
    LReLU'(x) = 
    \begin{cases}
        \alpha & \quad x \le 0 \\
        1 & \quad x > 0 \\
    \end{cases}
$$
> 通常 $\alpha \in (0.1, 0.3)$

## 7. SELU(扩展型指数线性单元)
> 能避免`梯度消失`、`梯度爆炸`问题  

$$
    SELU(x) =  \lambda 
    \begin{cases}
        \alpha e^x - \alpha & \quad x \le 0 \\
        x & \quad x > 0 \\
    \end{cases} \\
    SELU'(x) = \lambda
    \begin{cases}
        \alpha e^x & \quad x \le 0 \\
        1 & \quad x > 0 \\
    \end{cases} \\
$$
- 权重初始化，必须使用`lecun_normal`
- 如果使用dropout，必须使用`Alpha Dropout`

## 8. GELU(Gaussian Error Linear Unit)
> 似乎是NLP领域的当前最佳

- Transformer中使用
$$
    GELU(x) = 0.5x \left( 1 + \tanh \left( \sqrt{\frac 2 \pi} (x+0.044715x^3) \right) \right)
$$

- 代码实现
    ```
    def gelu(x):
        return 0.5 * x * (1 + np.tanh( np.sqrt(2/np.pi) * (x + 0.044715*np.power(x,3)) ))
    ```